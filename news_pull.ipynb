{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data from DAWN - 01 Jan 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"2020-01-01\"\n",
    "newspaper = \"Dawn\"\n",
    "url = \"https://www.dawn.com/archive/2020-01-01\"\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text, \"html5lib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = soup.find_all(\"h2\", {'data-layout': 'story'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are done!\n"
     ]
    }
   ],
   "source": [
    "dawn_csv = open(\"dawn.csv\", \"w\")\n",
    "columns = ['date', 'newspaper', 'title', 'data-id', 'link']\n",
    "fill = csv.DictWriter(dawn_csv,fieldnames=columns)\n",
    "fill.writeheader()\n",
    "for link in articles:\n",
    "    article_link = BeautifulSoup(str(link), 'html.parser')\n",
    "    getlink = article_link.find('a', href=True)\n",
    "    fill.writerow({'date': date, 'newspaper': newspaper, \n",
    "                   'title': str(getlink.find(text=True)), \n",
    "                   'data-id': str(getlink['href'].split(\"/\")[4]), \n",
    "                   'link': str(getlink['href'])})\n",
    "\n",
    "dawn_csv.close()\n",
    "\n",
    "print(\"We are done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newspaper</th>\n",
       "      <th>title</th>\n",
       "      <th>data-id</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>Dawn</td>\n",
       "      <td>Federation of Pakistan v Gen Pervez Musharraf:...</td>\n",
       "      <td>1525658</td>\n",
       "      <td>https://www.dawn.com/news/1525658/federation-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>Dawn</td>\n",
       "      <td>Chinese national held for beating traffic poli...</td>\n",
       "      <td>1525657</td>\n",
       "      <td>https://www.dawn.com/news/1525657/chinese-nati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>Dawn</td>\n",
       "      <td>Iraqi paramilitaries call for withdrawal from ...</td>\n",
       "      <td>1525656</td>\n",
       "      <td>https://www.dawn.com/news/1525656/iraqi-parami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>Dawn</td>\n",
       "      <td>Sarmad Khoosat reveals why Zindagi Tamasha's t...</td>\n",
       "      <td>1525655</td>\n",
       "      <td>https://www.dawn.com/news/1525655/sarmad-khoos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>Dawn</td>\n",
       "      <td>PSL 2020 set to begin on February 20\\r\\n</td>\n",
       "      <td>1525653</td>\n",
       "      <td>https://www.dawn.com/news/1525653/psl-2020-set...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           newspaper                                              title  \\\n",
       "date                                                                      \n",
       "2020-01-01      Dawn  Federation of Pakistan v Gen Pervez Musharraf:...   \n",
       "2020-01-01      Dawn  Chinese national held for beating traffic poli...   \n",
       "2020-01-01      Dawn  Iraqi paramilitaries call for withdrawal from ...   \n",
       "2020-01-01      Dawn  Sarmad Khoosat reveals why Zindagi Tamasha's t...   \n",
       "2020-01-01      Dawn           PSL 2020 set to begin on February 20\\r\\n   \n",
       "\n",
       "            data-id                                               link  \n",
       "date                                                                    \n",
       "2020-01-01  1525658  https://www.dawn.com/news/1525658/federation-o...  \n",
       "2020-01-01  1525657  https://www.dawn.com/news/1525657/chinese-nati...  \n",
       "2020-01-01  1525656  https://www.dawn.com/news/1525656/iraqi-parami...  \n",
       "2020-01-01  1525655  https://www.dawn.com/news/1525655/sarmad-khoos...  \n",
       "2020-01-01  1525653  https://www.dawn.com/news/1525653/psl-2020-set...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspapers_df = pd.read_csv('dawn.csv', index_col='date', engine='python')\n",
    "newspapers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: There are a few issues. First of all, the .csv file contains some articles that do not belong from the same dates. It contains some random articles from the front page or other parts of the website. One way to handle is it by ignoring and then let it get resolved at the stage where we use the link to extract the content of the article. \n",
    "An alternative way is to note that data-id is unique, however, there is no set pattern that reflect which day, month or year it refers to. Let's say we want to extract data set from 2012 to 2015. We can manually try to find what data-id correspond to the news artcile from 2012 and the last data-id corresponding to the news article in 2015. And, then do scrape iteratively. However, this would give articles that have been written by non-authors like AFP or Agencies etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
